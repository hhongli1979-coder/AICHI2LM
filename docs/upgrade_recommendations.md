# TeleChat é¡¹ç›®å‡çº§å»ºè®®

æœ¬æ–‡æ¡£åˆ†æäº†TeleChaté¡¹ç›®çš„å½“å‰çŠ¶æ€ï¼Œå¹¶æä¾›å…¨é¢çš„å‡çº§å»ºè®®ã€‚

## ç›®å½•
- [å½“å‰çŠ¶æ€è¯„ä¼°](#å½“å‰çŠ¶æ€è¯„ä¼°)
- [ç´§æ€¥å‡çº§é¡¹](#ç´§æ€¥å‡çº§é¡¹)
- [é‡è¦å‡çº§é¡¹](#é‡è¦å‡çº§é¡¹)
- [å¢å¼ºåŠŸèƒ½å»ºè®®](#å¢å¼ºåŠŸèƒ½å»ºè®®)
- [æ¶æ„ä¼˜åŒ–å»ºè®®](#æ¶æ„ä¼˜åŒ–å»ºè®®)
- [å‡çº§å®æ–½è·¯çº¿å›¾](#å‡çº§å®æ–½è·¯çº¿å›¾)

---

## å½“å‰çŠ¶æ€è¯„ä¼°

### é¡¹ç›®ç»“æ„æ¦‚è§ˆ

| æ¨¡å— | å½“å‰çŠ¶æ€ | è¯„åˆ† | å‡çº§ä¼˜å…ˆçº§ |
|------|----------|------|------------|
| æ¨ç†æœåŠ¡ | åŸºç¡€åŠŸèƒ½å®Œæ•´ | â­â­â­ | é«˜ |
| è®­ç»ƒæ¡†æ¶ | DeepSpeedæ”¯æŒ | â­â­â­â­ | ä¸­ |
| é‡åŒ–æ”¯æŒ | GPTQ 4/8bit | â­â­â­â­ | ä½ |
| APIæœåŠ¡ | FastAPIåŸºç¡€ç‰ˆ | â­â­ | é«˜ |
| è¯„æµ‹ç³»ç»Ÿ | MMLU/CEVAL | â­â­â­ | ä¸­ |
| æ–‡æ¡£ | åŸºç¡€å®Œå–„ | â­â­â­â­ | ä½ |

---

## ğŸš¨ ç´§æ€¥å‡çº§é¡¹ï¼ˆP0ï¼‰

### 1. ä¾èµ–ç‰ˆæœ¬å‡çº§

**å½“å‰é—®é¢˜ï¼š** `requirements.txt` ä¸­å¤šä¸ªä¾èµ–ç‰ˆæœ¬è¿‡æ—§ï¼Œå­˜åœ¨å®‰å…¨é£é™©å’Œå…¼å®¹æ€§é—®é¢˜ã€‚

```diff
# requirements.txt å»ºè®®å‡çº§

- torch==1.13.1
+ torch>=2.0.0

- transformers==4.30.0
+ transformers>=4.36.0

- deepspeed==0.8.3
+ deepspeed>=0.12.0

- uvicorn==0.17.6
+ uvicorn>=0.25.0

+ # æ–°å¢æ¨èä¾èµ–
+ vllm>=0.2.0          # é«˜æ€§èƒ½æ¨ç†
+ langchain>=0.1.0     # LLMåº”ç”¨æ¡†æ¶
+ openai>=1.0.0        # OpenAIå…¼å®¹API
```

### 2. APIæœåŠ¡å¢å¼º

**å½“å‰é—®é¢˜ï¼š** `telechat_service.py` ç¼ºå°‘å…³é”®åŠŸèƒ½

**å»ºè®®æ–°å¢ï¼š**

```python
# service/telechat_service_v2.py

from fastapi import FastAPI, HTTPException, Depends
from fastapi.security import HTTPBearer
from pydantic import BaseModel
import asyncio
from typing import Optional, List
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è¯·æ±‚æ¨¡å‹
class ChatRequest(BaseModel):
    messages: List[dict]
    max_tokens: Optional[int] = 2048
    temperature: Optional[float] = 0.7
    stream: Optional[bool] = False
    
class ChatResponse(BaseModel):
    id: str
    choices: List[dict]
    usage: dict

# APIç‰ˆæœ¬ç®¡ç†
app = FastAPI(
    title="TeleChat API",
    version="2.0.0",
    description="TeleChatå¤§æ¨¡å‹APIæœåŠ¡"
)

# å¥åº·æ£€æŸ¥ç«¯ç‚¹
@app.get("/health")
async def health_check():
    return {"status": "healthy", "model": "TeleChat-12B"}

# OpenAIå…¼å®¹æ¥å£
@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    """OpenAIå…¼å®¹çš„èŠå¤©æ¥å£"""
    pass

# æ‰¹é‡æ¨ç†æ¥å£
@app.post("/v1/batch")
async def batch_inference(requests: List[ChatRequest]):
    """æ‰¹é‡æ¨ç†æ¥å£ï¼Œæé«˜ååé‡"""
    pass

# æ¨¡å‹ä¿¡æ¯æ¥å£
@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return {
        "data": [
            {"id": "telechat-7b", "object": "model"},
            {"id": "telechat-12b", "object": "model"},
            {"id": "telechat-12b-v2", "object": "model"}
        ]
    }
```

### 3. é”™è¯¯å¤„ç†å¢å¼º

**å½“å‰é—®é¢˜ï¼š** å¼‚å¸¸å¤„ç†è¿‡äºç®€å•ï¼Œç¼ºå°‘è¯¦ç»†æ—¥å¿—

```python
# utils/error_handler.py

class TeleChatException(Exception):
    """TeleChatè‡ªå®šä¹‰å¼‚å¸¸"""
    def __init__(self, code: str, message: str, details: dict = None):
        self.code = code
        self.message = message
        self.details = details or {}
        super().__init__(self.message)

class ErrorCodes:
    PARAM_ERROR = "10001"
    MODEL_ERROR = "10002"
    INFERENCE_ERROR = "10003"
    MEMORY_ERROR = "10004"
    TIMEOUT_ERROR = "10005"

# å…¨å±€å¼‚å¸¸å¤„ç†å™¨
@app.exception_handler(TeleChatException)
async def telechat_exception_handler(request, exc):
    logger.error(f"TeleChat Error: {exc.code} - {exc.message}")
    return JSONResponse(
        status_code=400,
        content={
            "error": {
                "code": exc.code,
                "message": exc.message,
                "details": exc.details
            }
        }
    )
```

---

## âš ï¸ é‡è¦å‡çº§é¡¹ï¼ˆP1ï¼‰

### 4. æ¨ç†æ€§èƒ½ä¼˜åŒ–

**å»ºè®®æ–°å¢ vLLM æ¨ç†å¼•æ“æ”¯æŒï¼š**

```python
# inference_telechat/vllm_infer.py

from vllm import LLM, SamplingParams

class VLLMInference:
    """é«˜æ€§èƒ½vLLMæ¨ç†å¼•æ“"""
    
    def __init__(self, model_path: str, tensor_parallel_size: int = 1):
        self.llm = LLM(
            model=model_path,
            tensor_parallel_size=tensor_parallel_size,
            trust_remote_code=True,
            dtype="float16"
        )
        
    def generate(self, prompts: list, **kwargs):
        sampling_params = SamplingParams(
            temperature=kwargs.get("temperature", 0.7),
            top_p=kwargs.get("top_p", 0.9),
            max_tokens=kwargs.get("max_tokens", 2048)
        )
        outputs = self.llm.generate(prompts, sampling_params)
        return [output.outputs[0].text for output in outputs]
    
    def batch_generate(self, prompts: list, batch_size: int = 32):
        """æ‰¹é‡ç”Ÿæˆï¼Œæé«˜ååé‡"""
        results = []
        for i in range(0, len(prompts), batch_size):
            batch = prompts[i:i+batch_size]
            results.extend(self.generate(batch))
        return results
```

### 5. è®°å¿†ç³»ç»Ÿé›†æˆ

**é›†æˆ Memori å®ç°é•¿æœŸè®°å¿†ï¼š**

```python
# memory/telechat_memory.py

from memori import Memori

class TeleChatMemory:
    """TeleChatè®°å¿†ç³»ç»Ÿ"""
    
    def __init__(self, db_path: str = "telechat_memory.db"):
        self.memori = Memori(
            database_url=f"sqlite:///{db_path}",
            conscious_ingest=True
        )
        self.memori.enable()
        
    def store_conversation(self, user_id: str, messages: list):
        """å­˜å‚¨å¯¹è¯å†å²"""
        for msg in messages:
            self.memori.add_memory(
                content=msg["content"],
                metadata={
                    "user_id": user_id,
                    "role": msg["role"],
                    "timestamp": msg.get("timestamp")
                }
            )
    
    def retrieve_context(self, user_id: str, query: str, top_k: int = 5):
        """æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡"""
        memories = self.memori.search(
            query=query,
            filter={"user_id": user_id},
            top_k=top_k
        )
        return memories
    
    def get_user_profile(self, user_id: str):
        """è·å–ç”¨æˆ·ç”»åƒ"""
        return self.memori.get_entities(filter={"user_id": user_id})
```

### 6. å¤šè½®å¯¹è¯å¢å¼º

```python
# dialogue/multi_turn.py

class MultiTurnDialogueManager:
    """å¤šè½®å¯¹è¯ç®¡ç†å™¨"""
    
    def __init__(self, max_history: int = 10, max_tokens: int = 4096):
        self.max_history = max_history
        self.max_tokens = max_tokens
        self.sessions = {}
        
    def create_session(self, session_id: str):
        """åˆ›å»ºæ–°ä¼šè¯"""
        self.sessions[session_id] = {
            "history": [],
            "context": {},
            "created_at": datetime.now()
        }
        
    def add_turn(self, session_id: str, role: str, content: str):
        """æ·»åŠ å¯¹è¯è½®æ¬¡"""
        if session_id not in self.sessions:
            self.create_session(session_id)
            
        self.sessions[session_id]["history"].append({
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat()
        })
        
        # ä¿æŒå†å²é•¿åº¦é™åˆ¶
        self._trim_history(session_id)
        
    def get_context_prompt(self, session_id: str, current_query: str):
        """è·å–å¸¦ä¸Šä¸‹æ–‡çš„æç¤º"""
        history = self.sessions.get(session_id, {}).get("history", [])
        
        context_parts = []
        for turn in history[-self.max_history:]:
            if turn["role"] == "user":
                context_parts.append(f"ç”¨æˆ·: {turn['content']}")
            else:
                context_parts.append(f"åŠ©æ‰‹: {turn['content']}")
        
        context_parts.append(f"ç”¨æˆ·: {current_query}")
        return "\n".join(context_parts)
```

---

## ğŸ”§ å¢å¼ºåŠŸèƒ½å»ºè®®ï¼ˆP2ï¼‰

### 7. å‡½æ•°è°ƒç”¨æ”¯æŒ

```python
# tools/function_calling.py

class FunctionCallingEngine:
    """å‡½æ•°è°ƒç”¨å¼•æ“"""
    
    def __init__(self):
        self.registered_functions = {}
        
    def register_function(self, func, description: str, parameters: dict):
        """æ³¨å†Œå¯è°ƒç”¨å‡½æ•°"""
        self.registered_functions[func.__name__] = {
            "function": func,
            "description": description,
            "parameters": parameters
        }
        
    def generate_function_schema(self):
        """ç”Ÿæˆå‡½æ•°schemaä¾›æ¨¡å‹ä½¿ç”¨"""
        schemas = []
        for name, info in self.registered_functions.items():
            schemas.append({
                "name": name,
                "description": info["description"],
                "parameters": info["parameters"]
            })
        return schemas
    
    def execute_function(self, function_name: str, arguments: dict):
        """æ‰§è¡Œå‡½æ•°è°ƒç”¨"""
        if function_name not in self.registered_functions:
            raise ValueError(f"Unknown function: {function_name}")
        
        func = self.registered_functions[function_name]["function"]
        return func(**arguments)

# ç¤ºä¾‹ï¼šæ³¨å†Œæœç´¢å‡½æ•°
@function_calling_engine.register
def web_search(query: str) -> str:
    """æœç´¢ç½‘ç»œä¿¡æ¯"""
    # å®ç°æœç´¢é€»è¾‘
    pass

@function_calling_engine.register
def calculate(expression: str) -> float:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    return eval(expression)
```

### 8. RAGæ£€ç´¢å¢å¼º

```python
# rag/retrieval_augmented.py

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

class RAGEngine:
    """æ£€ç´¢å¢å¼ºç”Ÿæˆå¼•æ“"""
    
    def __init__(self, embedding_model: str = "BAAI/bge-large-zh-v1.5"):
        self.embedder = SentenceTransformer(embedding_model)
        self.index = None
        self.documents = []
        
    def add_documents(self, documents: list):
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        self.documents.extend(documents)
        embeddings = self.embedder.encode(documents)
        
        if self.index is None:
            dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)
        
        # å½’ä¸€åŒ–ç”¨äºä½™å¼¦ç›¸ä¼¼åº¦
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings)
        
    def retrieve(self, query: str, top_k: int = 5):
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        query_embedding = self.embedder.encode([query])
        faiss.normalize_L2(query_embedding)
        
        scores, indices = self.index.search(query_embedding, top_k)
        
        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.documents):
                results.append({
                    "document": self.documents[idx],
                    "score": float(scores[0][i])
                })
        return results
    
    def generate_with_context(self, query: str, model, tokenizer):
        """å¸¦æ£€ç´¢ä¸Šä¸‹æ–‡çš„ç”Ÿæˆ"""
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved = self.retrieve(query)
        
        # æ„å»ºå¢å¼ºæç¤º
        context = "\n".join([r["document"] for r in retrieved])
        augmented_prompt = f"""å‚è€ƒä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜ï¼š{query}

å›ç­”ï¼š"""
        
        # ç”Ÿæˆå›ç­”
        response = model.chat(tokenizer, augmented_prompt, history=[])
        return response
```

### 9. æµå¼è¾“å‡ºä¼˜åŒ–

```python
# service/streaming.py

import asyncio
from typing import AsyncGenerator

class StreamingHandler:
    """æµå¼è¾“å‡ºå¤„ç†å™¨"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        
    async def generate_stream(
        self, 
        prompt: str, 
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """å¼‚æ­¥æµå¼ç”Ÿæˆ"""
        generator = self.model.chat(
            self.tokenizer, 
            prompt, 
            stream=True,
            **kwargs
        )
        
        for token, _ in generator:
            if token:
                yield f"data: {json.dumps({'content': token})}\n\n"
                await asyncio.sleep(0)  # è®©å‡ºæ§åˆ¶æƒ
        
        yield "data: [DONE]\n\n"
    
    def format_sse(self, data: dict) -> str:
        """æ ¼å¼åŒ–ä¸ºSSEæ ¼å¼"""
        return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
```

---

## ğŸ—ï¸ æ¶æ„ä¼˜åŒ–å»ºè®®

### 10. é¡¹ç›®ç»“æ„é‡ç»„

**å»ºè®®çš„æ–°ç›®å½•ç»“æ„ï¼š**

```
AICHI2LM/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ telechat/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ model.py          # æ¨¡å‹åŠ è½½å’Œç®¡ç†
â”‚   â”‚   â”‚   â”œâ”€â”€ inference.py      # æ¨ç†å¼•æ“
â”‚   â”‚   â”‚   â””â”€â”€ tokenizer.py      # åˆ†è¯å™¨
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py         # APIè·¯ç”±
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas.py        # è¯·æ±‚/å“åº”æ¨¡å‹
â”‚   â”‚   â”‚   â””â”€â”€ middleware.py     # ä¸­é—´ä»¶
â”‚   â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”‚   â”œâ”€â”€ short_term.py     # çŸ­æœŸè®°å¿†
â”‚   â”‚   â”‚   â””â”€â”€ long_term.py      # é•¿æœŸè®°å¿†
â”‚   â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”‚   â”œâ”€â”€ function_calling.py
â”‚   â”‚   â”‚   â””â”€â”€ rag.py
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ config.py
â”‚   â”‚       â””â”€â”€ logging.py
â”‚   â””â”€â”€ training/
â”‚       â”œâ”€â”€ sft/                  # ç›‘ç£å¾®è°ƒ
â”‚       â”œâ”€â”€ rlhf/                 # RLHFè®­ç»ƒ
â”‚       â””â”€â”€ self_evolution/       # è‡ªè¿›åŒ–è®­ç»ƒ
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â””â”€â”€ integration/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ model_config.yaml
â”‚   â””â”€â”€ service_config.yaml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ start_service.sh
â”‚   â””â”€â”€ run_evaluation.sh
â””â”€â”€ docker/
    â”œâ”€â”€ Dockerfile
    â””â”€â”€ docker-compose.yml
```

### 11. é…ç½®ç®¡ç†ä¼˜åŒ–

```yaml
# configs/model_config.yaml

model:
  name: "TeleChat-12B-V2"
  path: "./models/12B-V2"
  dtype: "float16"
  device_map: "auto"
  
inference:
  max_length: 4096
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  
service:
  host: "0.0.0.0"
  port: 8080
  workers: 4
  timeout: 300
  
memory:
  enabled: true
  database_url: "sqlite:///telechat_memory.db"
  max_history: 20
  
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/telechat.log"
```

### 12. Dockeréƒ¨ç½²æ”¯æŒ

```dockerfile
# docker/Dockerfile

FROM nvidia/cuda:11.8-devel-ubuntu22.04

WORKDIR /app

# å®‰è£…Pythonå’Œä¾èµ–
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY src/ ./src/
COPY configs/ ./configs/

# æš´éœ²ç«¯å£
EXPOSE 8080

# å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "telechat.api.main"]
```

```yaml
# docker/docker-compose.yml

version: '3.8'

services:
  telechat-api:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    ports:
      - "8080:8080"
    volumes:
      - ../models:/app/models
      - ../logs:/app/logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - MODEL_PATH=/app/models/12B-V2
```

---

## ğŸ“‹ å‡çº§å®æ–½è·¯çº¿å›¾

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€å‡çº§ï¼ˆ1-2å‘¨ï¼‰

| ä»»åŠ¡ | æè¿° | è´Ÿè´£äºº | çŠ¶æ€ |
|------|------|--------|------|
| ä¾èµ–å‡çº§ | æ›´æ–°requirements.txt | - | â³ |
| APIå¢å¼º | æ·»åŠ å¥åº·æ£€æŸ¥ã€OpenAIå…¼å®¹æ¥å£ | - | â³ |
| é”™è¯¯å¤„ç† | å®Œå–„å¼‚å¸¸å¤„ç†å’Œæ—¥å¿— | - | â³ |
| é…ç½®ç®¡ç† | æ·»åŠ YAMLé…ç½®æ”¯æŒ | - | â³ |

### ç¬¬äºŒé˜¶æ®µï¼šæ€§èƒ½ä¼˜åŒ–ï¼ˆ2-4å‘¨ï¼‰

| ä»»åŠ¡ | æè¿° | è´Ÿè´£äºº | çŠ¶æ€ |
|------|------|--------|------|
| vLLMé›†æˆ | é«˜æ€§èƒ½æ¨ç†å¼•æ“ | - | â³ |
| æ‰¹é‡æ¨ç† | æ”¯æŒæ‰¹é‡è¯·æ±‚ | - | â³ |
| æµå¼ä¼˜åŒ– | SSEæµå¼å“åº”ä¼˜åŒ– | - | â³ |
| ç¼“å­˜æœºåˆ¶ | KVç¼“å­˜å’Œç»“æœç¼“å­˜ | - | â³ |

### ç¬¬ä¸‰é˜¶æ®µï¼šåŠŸèƒ½å¢å¼ºï¼ˆ4-8å‘¨ï¼‰

| ä»»åŠ¡ | æè¿° | è´Ÿè´£äºº | çŠ¶æ€ |
|------|------|--------|------|
| è®°å¿†ç³»ç»Ÿ | Memorié›†æˆ | - | â³ |
| RAGå¼•æ“ | æ£€ç´¢å¢å¼ºç”Ÿæˆ | - | â³ |
| å‡½æ•°è°ƒç”¨ | Toolä½¿ç”¨èƒ½åŠ› | - | â³ |
| å¤šæ¨¡æ€ | å›¾åƒç†è§£æ”¯æŒ | - | â³ |

### ç¬¬å››é˜¶æ®µï¼šè‡ªè¿›åŒ–èƒ½åŠ›ï¼ˆ8-12å‘¨ï¼‰

| ä»»åŠ¡ | æè¿° | è´Ÿè´£äºº | çŠ¶æ€ |
|------|------|--------|------|
| è‡ªè®­ç»ƒæ¡†æ¶ | å®ç°è‡ªæˆ‘è®­ç»ƒæœºåˆ¶ | - | â³ |
| è¿›åŒ–ç®—æ³• | è¾¾å°”æ–‡å“¥å¾·å°”æœºå®ç° | - | â³ |
| ç›‘æ§ç³»ç»Ÿ | è¿›åŒ–è¿‡ç¨‹ç›‘æ§ | - | â³ |

---

## å‡çº§ä¼˜å…ˆçº§æ€»ç»“

| ä¼˜å…ˆçº§ | å‡çº§é¡¹ | é¢„è®¡å·¥æ—¶ | å½±å“èŒƒå›´ |
|--------|--------|----------|----------|
| ğŸ”´ P0 | ä¾èµ–ç‰ˆæœ¬å‡çº§ | 2å¤© | å…¨å±€ |
| ğŸ”´ P0 | APIæœåŠ¡å¢å¼º | 5å¤© | æœåŠ¡å±‚ |
| ğŸ”´ P0 | é”™è¯¯å¤„ç† | 3å¤© | å…¨å±€ |
| ğŸŸ¡ P1 | vLLMæ¨ç† | 5å¤© | æ¨ç†å±‚ |
| ğŸŸ¡ P1 | è®°å¿†ç³»ç»Ÿ | 7å¤© | å¯¹è¯å±‚ |
| ğŸŸ¡ P1 | å¤šè½®å¯¹è¯ | 5å¤© | å¯¹è¯å±‚ |
| ğŸŸ¢ P2 | å‡½æ•°è°ƒç”¨ | 7å¤© | åŠŸèƒ½å±‚ |
| ğŸŸ¢ P2 | RAGå¼•æ“ | 10å¤© | çŸ¥è¯†å±‚ |
| ğŸŸ¢ P2 | Dockeréƒ¨ç½² | 3å¤© | éƒ¨ç½²å±‚ |

---

*æ–‡æ¡£åˆ›å»ºæ—¶é—´ï¼š2024å¹´*

*å»ºè®®æŒ‰ä¼˜å…ˆçº§é¡ºåºé€æ­¥å®æ–½å‡çº§*
