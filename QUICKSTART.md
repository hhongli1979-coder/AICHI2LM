# TeleChat å¿«é€Ÿå¼€å§‹

æœ¬æ–‡æ¡£æä¾› TeleChat æœ€å¸¸ç”¨å‘½ä»¤çš„å¿«é€Ÿå‚è€ƒã€‚è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ [å®Œæ•´å‘½ä»¤å‚è€ƒ](./COMMANDS.md)ã€‚

## ğŸš€ ä¸€é”®éƒ¨ç½²

```bash
# æœ€ç®€å•çš„æ–¹å¼ - ä¸€é”®å¯åŠ¨ API å’Œ Web æœåŠ¡
python deploy.py

# è®¿é—®æœåŠ¡
# API æ–‡æ¡£: http://localhost:8070/docs
# Web ç•Œé¢: http://localhost:8501
```

## ğŸ“¥ å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

## ğŸ’¬ å¿«é€Ÿæ¨ç†

```python
import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

os.environ["CUDA_VISIBLE_DEVICES"] = '0'
PATH = 'models/7B'  # æˆ– models/12B

# åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(PATH, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    PATH, trust_remote_code=True, device_map="auto", torch_dtype=torch.float16
)
generate_config = GenerationConfig.from_pretrained(PATH)

# å¯¹è¯
question = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
answer, history = model.chat(
    tokenizer=tokenizer, question=question, history=[],
    generation_config=generate_config, stream=False
)
print(answer)
```

## ğŸ¯ è®­ç»ƒæ¨¡å‹

```bash
cd deepspeed-telechat/sft

# 1. å¤„ç†æ•°æ®
python process_data.py \
    --data_path data.json \
    --tokenizer_path ../../models/12B \
    --data_output_path datas/data_files \
    --max_seq_len 4096 \
    --num_samples 10000 \
    --num_workers 10 \
    --process_method multiple

# 2. å•æœºå¤šå¡è®­ç»ƒ
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
bash run_telechat_single_node.sh
```

## âš¡ æ¨¡å‹é‡åŒ–

```python
from transformers import AutoTokenizer
from auto_gptq import BaseQuantizeConfig
from modeling_telechat_gptq import TelechatGPTQForCausalLM

# Int4 é‡åŒ–
tokenizer_path = 'models/7B'
pretrained_model_dir = 'models/7B'
quantized_model_dir = 'models/7B-int4'

tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
examples = [tokenizer("auto-gptq is an easy-to-use model quantization library.")]

quantize_config = BaseQuantizeConfig(bits=4, group_size=128, desc_act=False)
model = TelechatGPTQForCausalLM.from_pretrained(
    pretrained_model_dir, quantize_config, trust_remote_code=True
)
model.quantize(examples)
model.save_quantized(quantized_model_dir)
```

## ğŸ”§ å¸¸è§é…ç½®

### æŒ‡å®š GPU

```bash
# ä½¿ç”¨å•ä¸ª GPU
export CUDA_VISIBLE_DEVICES=0
python deploy.py --gpu 0

# ä½¿ç”¨å¤šä¸ª GPU
export CUDA_VISIBLE_DEVICES=0,1,2,3
python deploy.py --gpu 0,1,2,3
```

### æŒ‡å®šæ¨¡å‹è·¯å¾„

```bash
python deploy.py --model-path models/12B
```

### è‡ªå®šä¹‰ç«¯å£

```bash
python deploy.py --api-port 8080 --web-port 8502
```

## ğŸ“Š æ¨¡å‹è¯„æµ‹

```bash
cd evaluation

# C-Eval
wget https://huggingface.co/datasets/ceval/ceval-exam/resolve/main/ceval-exam.zip
unzip ceval-exam.zip
python score_CEVAL.py --path ../models/7B --five_shot

# MMLU
python score_MMLU.py
```

## ğŸŒ æœåŠ¡éƒ¨ç½²

### å¯åŠ¨ API æœåŠ¡

```bash
cd service
python telechat_service.py
# API æ–‡æ¡£: http://localhost:8070/docs
```

### å¯åŠ¨ Web ç•Œé¢

```bash
cd service
streamlit run web_demo.py
# Web ç•Œé¢: http://localhost:8501
```

## ğŸ“– æ›´å¤šä¿¡æ¯

- **å®Œæ•´å‘½ä»¤å‚è€ƒ**: [COMMANDS.md](./COMMANDS.md)
- **è¯¦ç»†æ•™ç¨‹**: [docs/tutorial.md](./docs/tutorial.md)
- **éƒ¨ç½²æŒ‡å—**: [DEPLOYMENT.md](./DEPLOYMENT.md)
- **æ¨¡å‹ä¸‹è½½**: https://huggingface.co/Tele-AI

## ğŸ†˜ å¸¸è§é—®é¢˜

### GPU å†…å­˜ä¸è¶³ï¼Ÿ

ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼š
```bash
python deploy.py --model-path models/7B-int4
```

### ç«¯å£è¢«å ç”¨ï¼Ÿ

ä½¿ç”¨å…¶ä»–ç«¯å£ï¼š
```bash
python deploy.py --api-port 8080 --web-port 8502
```

### æ¨¡å‹åŠ è½½å¤±è´¥ï¼Ÿ

æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´ï¼š
```bash
ls -lh models/7B/
# åº”è¯¥åŒ…å«: config.json, pytorch_model.bin, tokenizer.model ç­‰
```

---

**æç¤º**: æ‰€æœ‰å‘½ä»¤çš„è¯¦ç»†è¯´æ˜å’Œé«˜çº§ç”¨æ³•è¯·å‚è€ƒ [COMMANDS.md](./COMMANDS.md)
